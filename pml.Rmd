---
title: "Course Project - Practical Machine Learning"
author: "Alberto Mac√≠as"
date: "5/3/2021"
output: html_document
---

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F, include=F}
library(ggplot2)
library(dplyr)
library(caret)
library(e1071) # For svm
```

This work is the project for the Practical Machine Learning course by John Hopkins University on Coursera. Data was downloaded from the course page and for more information about it, it can be consulted [here](http://groupware.les.inf.puc-rio.br/har.).


## Feature selection and preprocessing

```{r}
training <- read.csv("./data/pml-training.csv", header = T)
```

In this part of the work, I use the "training" dataset provided by Coursera. The data set consists of 160 variables and over 19 thousand observation. There are a lot of variables that contains NA values or no values at all, but theese seem to be some measures, like mean or variance, about other variables, so I drop them.

```{r}
training$classe <- as.factor(training$classe)
training <- training[,-1:-7]
training <- training[,-5:-29]
training <- training[,-18:-27]
training <- training[,-27:-41]
training <- training[,-30:-44]
training <- training[,-31:-40]
training <- training[,-43:-57]
training <- training[,-44:-53]
```

For the cross validation, I split the data into train, test and validation sets. Validation set is about twenty percent of the total data, train is about seventy percent of the rest.

```{r}
set.seed(3455)
inBuild <- createDataPartition(training$classe, p=0.8, list = F)
buildData <- training[inBuild,]
validationData <- training[-inBuild,]
inTrain <- createDataPartition(buildData$classe, p=0.7, list = F)
trainData <- buildData[inTrain,]
testData <- buildData[-inTrain,]
```

The preprocessing part consists of a principal component analysis, in order to reduce the dimensions of data. Also, I normalize the data to prevent reduce bias and variance.

```{r}
set.seed(1212)
trainClasse <- trainData$classe; trainData <- trainData[,-53]
preproc <- preProcess(trainData, method = c("pca", "center", "scale"))
preprocessedData <- predict(preproc, trainData)
preprocessedData$classe <- trainClasse
```


### Model tunning

I start with three algorithms: random forest, linear discriminant analysis and support vector machine.
```{r, echo=F}
set.seed(22)
rfmodel <- train(classe~., method="rf", data = preprocessedData,
                 trControl= trainControl(method = "cv", number = 3))
ldamodel <- train(classe~., method="lda", data = preprocessedData)
svmmodel <- svm(classe~., data=preprocessedData)
```

I predict these models on the test set to evaluate out of sample error and I check the accuracy of these models on the test set.
```{r}
testClasse <- testData$classe; testData <- testData[,-53]
processedTestData <- predict(preproc, testData)
processedTestData$classe <- testClasse
predrf <- predict(rfmodel, processedTestData)
predlda <- predict(ldamodel, processedTestData)
predsvm <- predict(svmmodel, processedTestData)

print("Random Forest model Accuracy:")
confusionMatrix(testClasse, predrf)$overall["Accuracy"]
print("Linear Discriminant Analysis model Accuracy: ")
confusionMatrix(testClasse, predlda)$overall["Accuracy"]
print("Support Vector Machine model Accuracy:")
confusionMatrix(testClasse, predsvm)$overall["Accuracy"]
```

Random Forest and support vector machine seem to have a good performance, but linear discriminant analysis doesn't. That's why I decided to fit a boosting method instead.

```{r, include=F}
gmbmodel <- train(classe~., method="gbm", data = preprocessedData)
```

Boosting seems to have a better accuracy.
```{r}
predgbm <- predict(gmbmodel, processedTestData)
print("Boosting model Accuracy:")
confusionMatrix(testClasse, predgbm)$overall["Accuracy"]
```

Finally, I stack the three methods in order to improve a little the accuracy, so I trained in the predictions on the test data and train a random forest.
```{r}
predDF <- data.frame(predrf, predsvm, predgbm, classe=testClasse)
combModel <- train(classe~., method="rf", data=predDF,
                   trControl = trainControl("cv", number = 3))

valClasse <- validationData$classe
validationData <- validationData[,-53]
proccesedvalData <- predict(preproc, validationData)

predrf <- predict(rfmodel, proccesedvalData)
predsvm <- predict(svmmodel, proccesedvalData)
predgbm <- predict(gmbmodel, proccesedvalData)
predDF <- data.frame(predrf, predsvm, predgbm)

predComb <- predict(combModel, predDF)
```

Stacking these models seem to improve the accuracy. 
```{r}
print("Combined model Accuracy on validation data:")
(combined.model <- confusionMatrix(valClasse, predComb)$overall["Accuracy"])
```

### Comparing models

In order to select a model, I compare the accuracy of all models on the validation set.
```{r}
rf.model <- confusionMatrix(valClasse, predrf)$overall["Accuracy"]
svm.model <- confusionMatrix(valClasse, predsvm)$overall["Accuracy"]
gbm.model <- confusionMatrix(valClasse, predgbm)$overall["Accuracy"]
(models.accuracy <- data.frame(rf.model, svm.model, gbm.model, combined.model))
```

It seems that random forest and the combined models have similar accuracy, so just for simplicity I will choose the random forest model.

